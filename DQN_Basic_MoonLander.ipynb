{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import ptan\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "from lib import common\n",
    "\n",
    "import collections\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Priority Replay Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrioReplayBuffer:\n",
    "    def __init__(self, exp_source, buf_size, prob_alpha=0.6):\n",
    "        self.exp_source_iter = iter(exp_source)\n",
    "        self.prob_alpha = prob_alpha\n",
    "        self.capacity = buf_size\n",
    "        self.pos = 0\n",
    "        self.buffer = []\n",
    "        # each experience in the buffer will be assigned a \"priority\"\n",
    "        # initialize each priority to 0\n",
    "        # the priorities will be the loss for each experience, this\n",
    "        # will be used to set the probablity of selecting that \n",
    "        # experience in other batches, where the more wrong the loss\n",
    "        # the more likely it is to be selected, so it can train on it better\n",
    "        self.priorities = np.zeros((buf_size,), dtype=np.float32)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "    \n",
    "    def populate(self, count):\n",
    "        # what is the maximum priority in the buffer if it is not empty?\n",
    "        max_prio = self.priorities.max() if self.buffer else 1.0\n",
    "        \n",
    "        # sample \"count,\" experiences, and keep track of the oldest experience in the buffer\n",
    "        # by tracking it's position within the buffer\n",
    "        for _ in range(count):\n",
    "            sample = next(self.exp_source_iter) # generate/get the next experience (s,a,s',r)\n",
    "            \n",
    "            if len(self.buffer) < self.capacity:\n",
    "                self.buffer.append(sample) # append to the buffer if the buffer isn't full\n",
    "            else:\n",
    "                self.buffer[self.pos] = sample # replace the oldest experience in the buffer with the new one\n",
    "                \n",
    "            self.priorities[self.pos] = max_prio # set the priority of the current experience to the max so,\n",
    "                                                 # that its very likely to get sampled, since it hasn't\n",
    "                                                 # been seen before in any batch\n",
    "            \n",
    "            self.pos = (self.pos + 1) % self.capacity # cyclic the oldest xp index to the next one\n",
    "    \n",
    "    def sample(self, batch_size, beta=0.4):\n",
    "        \n",
    "        if len(self.buffer) == self.capacity:\n",
    "            prios = self.priorities\n",
    "        else:\n",
    "            prios = self.priorities[:self.pos]\n",
    "        \n",
    "        # convert priority numbers to probabilities via\n",
    "        # P(i) = priority_i**alpha/(sum(priority_i**alpha))\n",
    "        probs = prios ** self.prob_alpha\n",
    "        probs /= probs.sum()\n",
    "        \n",
    "        # randomly select from the experience buffer, a batch size sample, with probabilities\n",
    "        # of experience selection = probs\n",
    "        indices = np.random.choice(len(self.buffer), batch_size, p=probs, replace=True)\n",
    "        samples = [self.buffer[idx] for idx in indices]\n",
    "        \n",
    "        # because we pull from from a non-uniform sample\n",
    "        # we need compute the importance sampling weight for \n",
    "        # SGD to work, we kind of cheat here by adding a beta\n",
    "        # in pure importance sampling beta = 1\n",
    "        total = len(self.buffer)\n",
    "        weights = (total*probs[indices])**(-beta)\n",
    "        weights /= weights.max()\n",
    "        \n",
    "        # return the samples, the weights, and indices, where the indices\n",
    "        # are going to be used to update their priorities after the loss\n",
    "        # is computed on them\n",
    "        return(samples, indices, np.array(weights, dtype=np.float32))\n",
    "    \n",
    "    def update_priorities(self, batch_indices, batch_priorities):\n",
    "        for idx, prio in zip(batch_indices, batch_priorities):\n",
    "            self.priorities[idx] = prio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experience Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# each experience is a tuple (s,a,r,d,s')\n",
    "Experience = collections.namedtuple('Experience', \n",
    "                                    field_names=['state','action','reward','done','new_state'])\n",
    "\n",
    "# make a buffer to hold \"capacity\" number of experiences in a queue\n",
    "class ExperienceBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = collections.deque(maxlen=capacity)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "    \n",
    "    def append(self,experience):\n",
    "        self.buffer.append(experience)\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        # randomly pick from the experience buffer\n",
    "        indices = np.random.choice(len(self.buffer), batch_size, replace=False)\n",
    "        \n",
    "        states, actions, rewards, dones, next_states = \\\n",
    "        zip(*[self.buffer[idx] for idx in indices])\n",
    "        \n",
    "        return np.array(states),np.array(actions),np.array(rewards,dtype=np.float32),\\\n",
    "               np.array(dones),np.array(next_states)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# an agent has an env, an experience buffer, a Q(s,a) network\n",
    "class Agent:\n",
    "    def __init__(self, env, exp_buffer, net):\n",
    "        self.env = env\n",
    "        self.exp_buffer = exp_buffer\n",
    "        self.net = net\n",
    "        self._reset()\n",
    "    \n",
    "    def _reset(self):\n",
    "        self.state = env.reset()\n",
    "        self.total_reward = 0.0\n",
    "    \n",
    "    def play_step(self, epsilon=0.0, device=\"cpu\"):\n",
    "        done_reward = None\n",
    "        \n",
    "        # epsilon greedy play\n",
    "        if np.random.random() < epsilon:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            state_a = np.array([self.state], copy=False)\n",
    "            state_v = torch.tensor(state_a).to(device)\n",
    "            q_vals_v = self.net(state_v)\n",
    "            _,act_v = torch.max(q_vals_v,dim=1) # argmax_a Q(s,a)\n",
    "            action = int(act_v.item())\n",
    "        \n",
    "        # take the step\n",
    "        new_state, reward, is_done, _ = self.env.step(action)\n",
    "        self.total_reward += reward\n",
    "        \n",
    "        # create the experience object -(s,a,r,d,s')\n",
    "        exp = Experience(self.state, action, reward, is_done, new_state)\n",
    "        # append the experience to the buffer\n",
    "        self.exp_buffer.append(exp)\n",
    "        \n",
    "        # setup for the next iteration unless done\n",
    "        self.state = new_state\n",
    "        if is_done:\n",
    "            done_reward = self.total_reward\n",
    "            self._reset()\n",
    "            return done_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean Square Error Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MSE loss on priority replay\n",
    "def calc_loss(batch, net, tgt_net, gamma, device=\"cpu\"):\n",
    "    # grab a collection of random experiences\n",
    "    # E - {(s1,a1,r1,d1,s'1),(s2,a2,r2,d2,s'2),...}\n",
    "    states, actions, rewards, dones, next_states = batch\n",
    "    \n",
    "    # convert to pytorch vars\n",
    "    states_v = torch.tensor(states).to(device)\n",
    "    next_states_v = torch.tensor(next_states).to(device)\n",
    "    actions_v = torch.tensor(actions).to(device)\n",
    "    rewards_v = torch.tensor(rewards).to(device)\n",
    "    done_mask = torch.ByteTensor(dones).to(device)\n",
    "    \n",
    "    # Q(s,a) for each (s,a) pair in the batch \n",
    "    Q_sa = net(states_v).gather(1, actions_v.unsqueeze(-1)).squeeze(-1)\n",
    "    \n",
    "    # max_a' Q_tgt(s', a') for each s' in the batch\n",
    "    maxQ_sa = tgt_net(next_states_v).max(1)[0] # max in pytorch gives (values, indices) so you want part 0\n",
    "    \n",
    "    # terminal states dont have a max_a' Q_tgt(s',a'), the target for Q_sa is just r(s,a)\n",
    "    maxQ_sa[done_mask] = 0.0\n",
    "    \n",
    "    # build the Q(s,a) target\n",
    "    # y(s,a) = r(s,a) + gamma * max_a' Q_tgt(s',a')\n",
    "    y_sa = rewards_v + gamma*maxQ_sa.detach() # i use detach because I dont want the parameters of Q_tgt to change \n",
    "    \n",
    "    # sampling MSE loss,\n",
    "    # l_i = (Q(s_i,a_i) - y(s_i,a_i))**2\n",
    "    loss_v = (Q_sa - y_sa)**2\n",
    "    \n",
    "    #  L = 1/N sum_i(l_i), l_i + delta -- passed back for updating priorities\n",
    "    return(loss_v.mean(), loss_v + 1e-5) # avoid divide by zero issue with 1e-5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "env = gym.make('LunarLander-v2')\n",
    "\n",
    "\n",
    "params = {'run_name': 'lunarlander',\n",
    "         'epsilon_start': 1.0,\n",
    "         'epsilon_final': 0.02,\n",
    "         'epsilon_frames':   10**5,\n",
    "         'replay_size': 100000,\n",
    "         'replay_initial': 10000,\n",
    "         'batch_size': 32,\n",
    "         'gamma': 0.99,\n",
    "         'target_net_sync':1000,\n",
    "         }\n",
    "\n",
    "GAMMA = 0.95 # reward discount\n",
    "LEARNING_RATE = 0.001\n",
    "STOP_REWARD = 199 # 200 points to \"solve\" it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q(s,a) approximating function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_shape, n_actions):\n",
    "        super(DQN, self).__init__()\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(input_shape, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, n_actions)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "calc_loss() missing 1 required positional argument: 'gamma'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-e14f91dbbcc9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexp_buffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m     \u001b[0mloss_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalc_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_net\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m     \u001b[0mloss_t\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: calc_loss() missing 1 required positional argument: 'gamma'"
     ]
    }
   ],
   "source": [
    "EPSILON_FINAL = 0.01\n",
    "EPSILON_START = 1.0\n",
    "EPSILON_DECAY_LAST_FRAME = 100000\n",
    "REPLAY_START_SIZE = 1000\n",
    "\n",
    "# intial network\n",
    "net = DQN(env.observation_space.shape[0], env.action_space.n).to(device)\n",
    "# target network\n",
    "tgt_net = DQN(env.observation_space.shape[0], env.action_space.n).to(device)\n",
    "\n",
    "# experience buffer to hold recent experiences\n",
    "exp_buffer = ExperienceBuffer(params['replay_size'])\n",
    "\n",
    "# agent to play the game\n",
    "agent = Agent(env, exp_buffer, net)\n",
    "\n",
    "\n",
    "optimizer = optim.Adam(net.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "frame_idx = 0\n",
    "\n",
    "total_rewards = []\n",
    "best_mean_reward = None\n",
    "MEAN_REWARD_BOUND = 200\n",
    "SYNC_TARGET_FRAMES = 1000\n",
    "BATCH_SIZE = 256\n",
    "\n",
    "while True:\n",
    "    frame_idx += 1\n",
    "    \n",
    "    epsilon = max(EPSILON_FINAL, EPSILON_START - frame_idx / EPSILON_DECAY_LAST_FRAME)\n",
    "    reward = agent.play_step(epsilon, device=device)\n",
    "    \n",
    "    if reward is not None:\n",
    "        total_rewards.append(reward)\n",
    "        mean_reward = np.mean(total_rewards[-100:])\n",
    "        \n",
    "        if best_mean_reward is None or best_mean_reward < mean_reward:\n",
    "            if best_mean_reward is not None:\n",
    "                print(\"best mean reward updated %.3f -> %.3f\"%(best_mean_reward,mean_reward))\n",
    "                best_mean_reward = mean_reward\n",
    "        \n",
    "        if mean_reward > MEAN_REWARD_BOUND:\n",
    "            print(\"Solved in %d frames\"%frame_idx)\n",
    "            break\n",
    "    \n",
    "    # dont start training until we have a full replay queue\n",
    "    if len(exp_buffer) < REPLAY_START_SIZE:\n",
    "        continue\n",
    "    \n",
    "    # match the target network to the current one\n",
    "    if frame_idx % SYNC_TARGET_FRAMES == 0:\n",
    "        tgt_net.load_state_dict(net.state_dict())\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    batch = exp_buffer.sample(BATCH_SIZE)\n",
    "    loss_t = calc_loss(batch, net, tgt_net, device=device)\n",
    "    loss_t.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(frames[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_pytorch)",
   "language": "python",
   "name": "conda_pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
